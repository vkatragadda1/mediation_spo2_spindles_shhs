{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae75849-9461-400c-82c2-82b00c90599e",
   "metadata": {},
   "source": [
    "## SpO2 Features Extraction\n",
    "This code calculates all necessary metrics for SpO2 and respiration rate features from edf files and XML annotation files for participants. Outputs data to a csv. Note that the function requires edf files to be stored in a folder titled \"clean-edf\" and annotations to be stored in \"edf-annotations\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136644da-a456-4d0c-a8ff-b673005f90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary functions and packages \n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "import neurokit2 as nk\n",
    "import mne\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from scipy.signal import resample\n",
    "import xmltodict\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, 'hypoxic_burden')\n",
    "from hb_functions import detect_oxygen_desaturation, calc_hypoxic_burden, get_spo2_no_desat, filter_spo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb53b26c-d2b5-4d2c-84d9-3ef7e9f6d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sleep metrics based on sleep stages. Takes an array of stages and returns a dictionary, r, with sleep stages \n",
    "# tib = time in bed \n",
    "# tst = total sleep time \n",
    "# se = sleep efficiency \n",
    "# sl = sleep latency \n",
    "\n",
    "# Sleep macrostructure extraction from sleep stage array\n",
    "def get_macrostructures(sleep_stages, epoch_time=30):\n",
    "    # Assumes sleep_stages is between lights off to lights on only\n",
    "    # Assumes epoch level\n",
    "\n",
    "    # First part pulls out tib, sl, and waso \n",
    "    r = {}\n",
    "    r['tib'] = len(sleep_stages)*epoch_time/3600\n",
    "    sleep_ids = np.where(np.in1d(sleep_stages, [1,2,3,4]))[0] \n",
    "    r['tst'] = len(sleep_ids)*epoch_time/3600\n",
    "    r['se']  = r['tst']/r['tib']*100\n",
    "    if len(sleep_ids)>0:\n",
    "        r['waso'] = np.sum(sleep_stages[sleep_ids[0]:sleep_ids[-1]+1]==5)*epoch_time/60\n",
    "        r['sl'] = sleep_ids[0]*epoch_time/60\n",
    "    else:\n",
    "        r['waso'] = np.nan\n",
    "        r['sl'] = np.nan\n",
    "    rem_ids = np.where(sleep_stages==4)[0]\n",
    "    if len(rem_ids)>0:\n",
    "        r['rl'] = rem_ids[0]*epoch_time/60\n",
    "    else:\n",
    "        r['rl'] = np.nan\n",
    "\n",
    "    # Calculates time in each stage  \n",
    "    r['w_time'] = (sleep_stages==5).sum()*epoch_time/60\n",
    "    r['r_time'] = (sleep_stages==4).sum()*epoch_time/60\n",
    "    r['n1_time'] = (sleep_stages==3).sum()*epoch_time/60\n",
    "    r['n2_time'] = (sleep_stages==2).sum()*epoch_time/60\n",
    "    r['n3_time'] = (sleep_stages==1).sum()*epoch_time/60\n",
    "\n",
    "    # Percent time in each stage \n",
    "    r['r_perc'] = (sleep_stages==4).mean()*100\n",
    "    r['n1_perc'] = (sleep_stages==3).mean()*100\n",
    "    r['n2_perc'] = (sleep_stages==2).mean()*100\n",
    "    r['n3_perc'] = (sleep_stages==1).mean()*100\n",
    "\n",
    "    # Sleep stages separated by first and second half of the night \n",
    "    Lhalf = len(sleep_stages)//2\n",
    "    r['r_perc_half1'] = (sleep_stages[:Lhalf]==4).mean()*100\n",
    "    r['n1_perc_half1'] = (sleep_stages[:Lhalf]==3).mean()*100\n",
    "    r['n2_perc_half1'] = (sleep_stages[:Lhalf]==2).mean()*100\n",
    "    r['n3_perc_half1'] = (sleep_stages[:Lhalf]==1).mean()*100\n",
    "    r['r_perc_half2'] = (sleep_stages[Lhalf:]==4).mean()*100\n",
    "    r['n1_perc_half2'] = (sleep_stages[Lhalf:]==3).mean()*100\n",
    "    r['n2_perc_half2'] = (sleep_stages[Lhalf:]==2).mean()*100\n",
    "    r['n3_perc_half2'] = (sleep_stages[Lhalf:]==1).mean()*100\n",
    "    \n",
    "    # Transition probability matrix \n",
    "    # Calculates transitions between stages, converts counts into probabilities to account for chance of stage transition happening in a certain stage\n",
    "    transmat = np.zeros((5,5))\n",
    "    for i in range(len(sleep_stages)-1):\n",
    "        s1 = sleep_stages[i]\n",
    "        s2 = sleep_stages[i+1]\n",
    "        if s1 in [1,2,3,4,5] and s2 in [1,2,3,4,5]:\n",
    "            transmat[int(s1)-1,int(s2)-1] += 1\n",
    "    transmat = transmat/transmat.sum(axis=1, keepdims=True)\n",
    "    r['n3_continue_prob'] = transmat[0,0]\n",
    "    r['n2_continue_prob'] = transmat[1,1]\n",
    "    r['n1_continue_prob'] = transmat[2,2]\n",
    "    r['r_continue_prob'] = transmat[3,3]\n",
    "    r['w_continue_prob'] = transmat[4,4]\n",
    "    \n",
    "    # Bout duration: longest continuous period spent in each sleep stage \n",
    "    ss = np.array(sleep_stages)\n",
    "    ss[np.isnan(ss)|np.isinf(ss)] = -1\n",
    "    r['n1_bout_dur'] = [0]\n",
    "    r['n2_bout_dur'] = [0]\n",
    "    r['n3_bout_dur'] = [0]\n",
    "    r['r_bout_dur'] = [0]\n",
    "    for k, l in groupby(ss):\n",
    "        if k==1:\n",
    "            r['n3_bout_dur'].append(len(list(l))*epoch_time/60)\n",
    "        elif k==2:\n",
    "            r['n2_bout_dur'].append(len(list(l))*epoch_time/60)\n",
    "        elif k==3:\n",
    "            r['n1_bout_dur'].append(len(list(l))*epoch_time/60)\n",
    "        elif k==4:\n",
    "            r['r_bout_dur'].append(len(list(l))*epoch_time/60)\n",
    "    r['n1_bout_dur'] = max(r['n1_bout_dur'])\n",
    "    r['n2_bout_dur'] = max(r['n2_bout_dur'])\n",
    "    r['n3_bout_dur'] = max(r['n3_bout_dur'])\n",
    "    r['r_bout_dur'] = max(r['r_bout_dur'])\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f982d386-92b7-4838-8e05-29fbd927644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate respiration rate (RR) from chest and abdomen signals using neurokit2\n",
    "\n",
    "def get_resp_rate(abd, chest, Fs):\n",
    "    \"\"\"\n",
    "    function to calculate RR for each PSG\n",
    "    abd = abdomen signal, array \n",
    "    chest = chest signal, array \n",
    "    Fs = sampling frequency in Hz of the two signals \n",
    "\n",
    "    Uses the neurokit library to make these calculations\n",
    "    \"\"\"\n",
    "\n",
    "    # First initiatlize empty lists \n",
    "    resp_rates = []\n",
    "    missing_rates = []\n",
    "\n",
    "    # Loop through the signals\n",
    "    # Calculate instantaneous respiration rate based on time between peaks\n",
    "    for resp in [abd, chest]:\n",
    "        try:\n",
    "            resp = nk.rsp_clean(resp, sampling_rate=Fs)\n",
    "            _, resp_peaks_dict = nk.rsp_peaks(resp, sampling_rate=Fs)\n",
    "            resp_peaks_dict = nk.rsp_fixpeaks(resp_peaks_dict)\n",
    "            resp_rate = nk.rsp_rate(resp, resp_peaks_dict, sampling_rate=Fs)\n",
    "            resp_rate[(resp_rate>30)|(resp_rate<5)] = np.nan  # remove extreme values\n",
    "        except Exception as ee:\n",
    "            resp_rate = np.zeros_like(resp)+np.nan\n",
    "        missing_rates.append(np.isnan(resp_rate).mean())\n",
    "        resp_rates.append(resp_rate)\n",
    "    resp_rate = resp_rates[np.argmin(missing_rates)]\n",
    "    return resp_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e4284e-5fa9-4109-bb35-9aabcc2dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that applies all pre-defined functions to each edf file and extracts SpO2 features for each participant from edf files \n",
    "# Stores results in CSV \n",
    "\n",
    "def main():\n",
    "    # Import mastersheet for demographics and PIDs\n",
    "    df = pd.read_excel('mastersheet_temp.xlsx')\n",
    "\n",
    "    # Create folders for storing all data generated \n",
    "    res_dir = 'intermediate_results'\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "    pickle_folder =\"clean-pickle\"\n",
    "    os.makedirs(pickle_folder, exist_ok=True)\n",
    "\n",
    "    # Extracting the PID, age, sex, etc \n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            age = df.Age.iloc[i]\n",
    "            sex = df.Sex.iloc[i]\n",
    "            pid = df.PID.iloc[i]\n",
    "            edf_path = os.path.join('clean-edf', f'shhs1-{pid}.edf')\n",
    "            print(pid)\n",
    "            \n",
    "            edf = mne.io.read_raw_edf(edf_path, verbose=False, preload=False)\n",
    "            # Note: when loading an EDF using the mne package, all signals are resampled to the highest frequency\n",
    "            channel_names = edf.info['ch_names']\n",
    "            start_time = edf.info['meas_date'].replace(tzinfo=None)\n",
    "\n",
    "            # Load only the SpO2 data \n",
    "            exclude_list = [x for x in channel_names if x!='SaO2']\n",
    "            edf_spo2 = mne.io.read_raw_edf(edf_path, verbose=False, exclude=exclude_list)\n",
    "            \n",
    "            data = edf_spo2.get_data()\n",
    "            ## NOTE: for 91 files, two additional channels were stored for SpO2. Airflow 0 and Airflow 1. \n",
    "\n",
    "            spo2 = data[0]  # shape = (T,)\n",
    "\n",
    "            # Set sampling frequency and resample to 1Hz \n",
    "            Fs = edf_spo2.info['sfreq']\n",
    "            if Fs!=1:\n",
    "                spo2 = spo2[::int(Fs)]\n",
    "                Fs = 1\n",
    "\n",
    "            # Load respiratory effort belt\n",
    "            resp_ch_names = ['THOR RES', 'ABDO RES']\n",
    "            edf_resp = mne.io.read_raw_edf(edf_path, verbose=False, exclude=[x for x in channel_names if x not in resp_ch_names ])\n",
    "            resp = edf_resp.get_data(picks=resp_ch_names)  # shape = (2,T)\n",
    "            Fs_resp = edf_resp.info['sfreq']\n",
    "            \n",
    "            # Resmaple to 10Hz if not 10Hz\n",
    "            if Fs_resp!=10:\n",
    "                resp = resample(resp, int(round(resp/Fs_resp*10)), axis=-1)\n",
    "                Fs_resp = 10\n",
    "            thor_res = resp[0]\n",
    "            abdo_res = resp[1]\n",
    "            chest = thor_res\n",
    "            abd = abdo_res\n",
    "            \n",
    "            # Read annotation files to extract scored sleep stage info  \n",
    "            annot_path = f'edf-annotations/shhs1-{pid}-nsrr.xml'\n",
    "            with open(annot_path, 'r') as f:\n",
    "                annot = xmltodict.parse(f.read())\n",
    "            annot = pd.DataFrame(annot['PSGAnnotation']['ScoredEvents']['ScoredEvent'])  \n",
    "            # annot is a pandas dataframe with columns: EventType, EventConcept, Start, Duration...\n",
    "\n",
    "            annot = annot[annot.EventType=='Stages|Stages'].reset_index(drop=True)\n",
    "            annot = annot.rename(columns={'Start':'onset', 'Duration':'duration', 'EventConcept':'description'})\n",
    "\n",
    "            # Creates a map to apply to annotations \n",
    "            stage_mapping = {'Wake|0':5, 'REM sleep|5':4,  'Stage 1 sleep|1':3, 'Stage 2 sleep|2':2, 'Stage 3 sleep|3':1} \n",
    "            annot['description'] = [stage_mapping.get(annot.description.iloc[x], np.nan) for x in range(len(annot.description))]\n",
    "            assert Fs==1 \n",
    "            assert Fs_resp==10\n",
    "            \n",
    "            # Essentially maps sleep stages from first function to match the same frequency of data points as SPO2 data is given \n",
    "            sleep_stages = np.zeros(len(spo2)) + np.nan\n",
    "            \n",
    "            # Create reverse mapping to safely check if description is a valid sleep stage number.\n",
    "            # This avoids issues with NaN or unexpected values in 'description' that may not match stage_mapping.values() cleanly.\n",
    "            reverse_mapping = {v: k for k, v in stage_mapping.items()}\n",
    "            \n",
    "            annot['onset'] = pd.to_numeric(annot['onset'], errors='coerce')\n",
    "            annot['duration']=pd.to_numeric(annot['duration'], errors='coerce')                               \n",
    "        \n",
    "            for j in range(len(annot)):\n",
    "                description = annot.loc[j, 'description']\n",
    "                \n",
    "                if description in reverse_mapping: \n",
    "                    start = int(round(annot.loc[j, 'onset']*Fs))\n",
    "                    end = int(round((annot.loc[j,'onset']+annot.loc[j, 'duration'])*Fs))\n",
    "                    start = max(0,start)\n",
    "                    end = min(len(sleep_stages),end)\n",
    "                    \n",
    "                    if start<end:\n",
    "                        sleep_stages[start:end] = description\n",
    "    \n",
    "    \n",
    "            # Now must downsample to standardize to the 30-second epochs \n",
    "            sleep_stages2 = sleep_stages[::30]\n",
    "    \n",
    "            # Use the first function to calculate the sleep macrostructures using the mapped sleep stage-SPO2 dataset \n",
    "            feat1 = get_macrostructures(sleep_stages2)\n",
    "            tst = feat1['tst']\n",
    "            \n",
    "            # Calculate hypoxic burden  \n",
    "            # Filter out the signal to suppress detailed output (limiting any noise and artifacts with verbose) \n",
    "            # Outputs info about the start time, duration, and how low it drops \n",
    "            feat2 = {}\n",
    "            spo2 = filter_spo2(spo2, Fs, verbose=False) #using func defined in py file, limiting extreme drops in spo2 signal \n",
    "            try:\n",
    "                od = detect_oxygen_desaturation(spo2, is_plot=False, max_duration=90)\n",
    "            except Exception as ee:\n",
    "                print(pid, str(ee))\n",
    "                od = []\n",
    "    \n",
    "            # Calculates hypoxic burden if desats present \n",
    "            # Calculates midpoints of desats using a function from hb_functions\n",
    "            # Otherwise assigns NAN to prevent errors \n",
    "            if len(od)>0:\n",
    "                event_times = od.Start.values+od.Duration.values/2\n",
    "                feat2['hb_desat'], hb_response_desat = calc_hypoxic_burden(spo2, event_times, 1, tst)\n",
    "                spo2_nodesat = get_spo2_no_desat(spo2, od)\n",
    "            else:\n",
    "                feat2['hb_desat'] = np.nan\n",
    "                hb_response_desat = np.nan\n",
    "                spo2_nodesat = spo2\n",
    "    \n",
    "            # Next, compute SPO2 values for each sleep stage \n",
    "            # ss is sleep stage/category, creating a boolean mask with ids to select correspnding data points \n",
    "            # sn is sleep number\n",
    "            # Go through each sleep stage of interest//mask others using bool//look at the relevant SpO2 values in that stage \n",
    "            for sn, ss in zip([0,1,2,3,4,5,[1,2,3],6,[1,2,3,4],7],['ALL', 'N3', 'N2', 'N1', 'R', 'W','NREM','WBSO','SLEEP','AFTER_SO']):\n",
    "                if type(sn)==list:\n",
    "                    ids = np.in1d(sleep_stages, sn)\n",
    "                elif sn==6:\n",
    "                    sleep_indices = np.where(np.in1d(sleep_stages, [1,2,3,4]))[0]\n",
    "                    if len(sleep_indices)>0:\n",
    "                        ids=np.arange(sleep_indices[0])\n",
    "                    else:\n",
    "                        ids = []\n",
    "                elif sn==7:\n",
    "                    sleep_indices = np.where(np.in1d(sleep_stages, [1,2,3,4]))[0]\n",
    "                    if len(sleep_indices)>0:\n",
    "                        ids=np.arange(sleep_indices[0], len(sleep_stages))\n",
    "                    else:\n",
    "                        ids = []\n",
    "                elif sn==0:\n",
    "                    ids = np.ones(len(sleep_stages), dtype=bool)\n",
    "                else:\n",
    "                    ids = sleep_stages==sn\n",
    "    \n",
    "                feat2['%spo2<95%_'+ss] = np.nanmean(spo2[ids]<95)*100 # SPO2 levels below 95% \n",
    "                feat2['%spo2<90%_'+ss] = np.nanmean(spo2[ids]<90)*100 # SPO2 below 90%\n",
    "                feat2['avg_spo2_'+ss] = np.nanmean(spo2[ids]) # AVG spo2 \n",
    "                feat2['avg_spo2_no_desat_'+ss] = np.nanmean(spo2_nodesat[ids]) # AVG no desats\n",
    "            \n",
    "            # Calculate differences between SPO2 metrics for after sleep onset versus before sleep onset\n",
    "            # Provides baseline and insight into SDB (sleep-disordered breathing)\n",
    "            for fn in ['%spo2<95%', '%spo2<90%','avg_spo2','avg_spo2_no_desat']:\n",
    "                feat2[fn+'_diff'] = feat2[fn+'_AFTER_SO']-feat2[fn+'_WBSO']\n",
    "    \n",
    "            # Respiration rate calculations \n",
    "            # Ensuring the same resolution (repeating the sleepstages data to match the respiration signal frequency)\n",
    "            # Calculates using a hb_Functions function \n",
    "            sleep_stages2 = np.repeat(sleep_stages, 10)\n",
    "            L = min(len(abd), len(sleep_stages2))\n",
    "            abd = abd[:L]\n",
    "            chest = chest[:L]\n",
    "            sleep_stages2 = sleep_stages2[:L]\n",
    "            resp_rate = get_resp_rate(abd, chest, Fs_resp)\n",
    "            \n",
    "            with open(os.path.join(res_dir, f'{pid}.pickle'), 'wb') as fff:\n",
    "                pickle.dump({\n",
    "                    'hb_response_desat':hb_response_desat,\n",
    "                    'spo2':spo2, 'spo2_nodesat':spo2_nodesat,\n",
    "                    'resp_rate':resp_rate,\n",
    "                    'sleep_stages':sleep_stages,\n",
    "                    }, fff)\n",
    "    \n",
    "            # Similar logic as above, calculating RR metrics per stage \n",
    "            feat3 = {}\n",
    "            for sn, ss in zip([0,1,2,3,4,5,[1,2,3],6,[1,2,3,4],7],['ALL', 'N3', 'N2', 'N1', 'R', 'W','NREM','WBSO','SLEEP','AFTER_SO']):\n",
    "                if type(sn)==list:\n",
    "                    ids = np.in1d(sleep_stages2, sn)\n",
    "                elif sn==6:\n",
    "                    ids = np.arange(np.where(np.in1d(sleep_stages2, [1,2,3,4]))[0][0])\n",
    "                elif sn==7:\n",
    "                    ids = np.arange(np.where(np.in1d(sleep_stages2, [1,2,3,4]))[0][0], len(sleep_stages2))\n",
    "                elif sn==0:\n",
    "                    ids = np.ones(len(sleep_stages2), dtype=bool)\n",
    "                else:\n",
    "                    ids = sleep_stages2==sn\n",
    "                feat3['avg_rr_'+ss] = np.nanmean(resp_rate[ids])\n",
    "            feat3['avg_rr_diff'] = feat3['avg_rr_AFTER_SO']-feat3['avg_rr_WBSO']\n",
    "    \n",
    "            # Merge the three feature dictionaries into one to export \n",
    "            # Keys k become column names, i is the row for each feature per participant\n",
    "            # \"items\" pulls out every key with every value \n",
    "            # Values become v for each feature \n",
    "            feats = feat1|feat2|feat3\n",
    "            for k,v in feats.items():\n",
    "                df.loc[i, k] = v\n",
    "        \n",
    "            # Starts with relevant participant metadata, then adds column names for all feature keys \n",
    "            # Reorders df to follow this order \n",
    "            cols = ['PID', 'Age', 'Sex', 'race']+list(feats.keys())\n",
    "            df = df[cols]\n",
    "        \n",
    "            # Save data to csv \n",
    "            df.to_csv('all_spo2_features.csv', index=False)\n",
    "            print(f\"Completed: {pid}\")\n",
    "        except Exception as outer_e:\n",
    "            print(f\"Error in processing PID {pid}: {outer_e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d08a15-ce5b-4ee5-9f91-3170db40fcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run function \n",
    "# Note: must have edf files stored in a folder titled \"clean-edf\"\n",
    "# Must have annotations in a file stored \"edf-annotations\"\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
